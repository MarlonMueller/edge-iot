#pragma once

#include <stdint.h>

#include "dl_layer_model.hpp"
#include "dl_layer_conv2d.hpp"
#include "dl_layer_softmax.hpp"
#include "dl_layer_flatten.hpp"
#include "dl_layer_max_pool2d.hpp"
#include "dl_layer_fullyconnected.hpp"

#include "{{ model_name }}_coefficient.hpp"

#define TAG "BIRDNET"

using namespace dl;
using namespace layer;
using namespace {{ model_name }}_coefficient;

// input_exponent: {{ input_exponent }}

class {{ model_name | upper }} : public Model<{{ quantization_bit }}_t>
{
private:

    {% for l in layers -%}
    {% if not loop.last -%}
    {%if l['layer'] == "conv2d" -%}
    Conv2D<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "maxpool2d" -%}
    MaxPool2D<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "flatten" -%}
    Flatten<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "fc" -%}
    FullyConnected<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "softmax" -%}
    Softmax<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% endif -%}
    {%- endif -%}
    {% endfor %}

public:

    {% for l in layers -%}
    {% if loop.last -%}
    {%if l['layer'] == "conv2d" -%}
    Conv2D<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "maxpool2d" -%}
    MaxPool2D<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "flatten" -%}
    Flatten<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "fc" -%}
    FullyConnected<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% elif l['layer'] == "softmax" -%}
    Softmax<{{ quantization_bit }}_t> {{ l['layer_name'] }};
    {% endif -%}
    {%- endif -%}
    {%- endfor %}

    {{ model_name | upper }}() :
    {%- for l in layers %}

        {%- if loop.last %}
        {% set delimiter = '{}' %}
        {%- else %}
        {% set delimiter = ',' %}
        {%- endif -%}
        

        {%if l['layer'] == "conv2d" -%}
        {{ l['layer_name'] }}(Conv2D<{{ quantization_bit }}_t>(
            {{ l['output_exponent'] }},
            get_statefulpartitionedcall_sequential_{{ l['layer_name'] }}_biasadd_filter(),
            get_statefulpartitionedcall_sequential_{{ l['layer_name'] }}_biasadd_bias(),
            get_statefulpartitionedcall_sequential_{{ l['layer_name'] }}_biasadd_activation(),
            {{ l['padding_type'] }}, {{ l['padding'] }},
            {{ l['stride_y'] }}, {{ l['stride_x'] }},
            "{{ l['layer_name'] }}")
        ){{ delimiter }}
        {% elif l['layer'] == "maxpool2d" -%}
        {{ l['layer_name'] }}(MaxPool2D<{{ quantization_bit }}_t>({{ l['filter_shape'] }}, {{ l['padding_type'] }}, {{ l['padding'] }}, {{ l['stride_y'] }}, {{ l['stride_x'] }}, "{{ l['layer_name'] }}")){{ delimiter }}
        {% elif l['layer'] == "flatten" -%}
        {{ l['layer_name'] }}(Flatten<{{ quantization_bit }}_t>("{{ l['layer_name'] }}", {{ l['inplace'] }})){{ delimiter }}
        {% elif l['layer'] == "fc" -%}
        {{ l['layer_name'] }}(FullyConnected<{{ quantization_bit }}_t>(
            {{ l['output_exponent'] }},
            get_fused_gemm_{{ l['layer_number'] }}_filter(),
            get_fused_gemm_{{ l['layer_number'] }}_bias(),
            {{ 'get_fused_gemm_' + l['layer_number'] + '_activation()' if l['activation'] else 'NULL' }},
            {{ l['flatten'] }}, "{{ l['layer_name'] }}")
        ){{ delimiter }}
        {% elif l['layer'] == "softmax" -%}
        {{ l['layer_name'] }}(Softmax<{{ quantization_bit }}_t>({{ l['output_exponent'] }}, "{{ l['layer_name'] }}")){{ delimiter }}
        {% endif -%}

    {% endfor %}


    void build(Tensor<{{ quantization_bit }}_t>& input) {
        {% for l in layers -%}
        {% if loop.first %}
        this->{{ l['layer_name'] }}.build(input, false);
        {% else %}
        this->{{ l['layer_name'] }}.build(this->{{ l['previous_layer_name'] }}.get_output(), false);
        {% endif %}
        {%-  endfor %}
    }

    void call(Tensor<{{ quantization_bit }}_t>& input) {
        
        {% for l in layers -%}
        {% if loop.first %}
        this->{{ l['layer_name'] }}.call(input);
        #ifdef CONFIG_HEAP_LOG
        ESP_LOGI(TAG, " Called {{ l['layer_name'] }}");
        log_heap();
        #endif
        input.free_element();
        #ifdef CONFIG_HEAP_LOG
        ESP_LOGI(TAG, " Freed input");
        log_heap();
        #endif
        {% else %}
        this->{{ l['layer_name'] }}.call(this->{{ l['previous_layer_name'] }}.get_output());
        #ifdef CONFIG_HEAP_LOG
        ESP_LOGI(TAG, " Called {{ l['layer_name'] }}");
        log_heap();
        #endif
        this->{{ l['previous_layer_name'] }}.get_output().free_element();
        #ifdef CONFIG_HEAP_LOG
        ESP_LOGI(TAG, " Freed {{ l['previous_layer_name'] }}");
        log_heap();
        #endif
        {% endif %}
        {%- endfor %}
    }
};